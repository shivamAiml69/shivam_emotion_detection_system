{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54344c92-283f-4b66-aeec-62db950caab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29a1d0e1-8ca6-4ce8-81a1-a391ea74bcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"D:\\welcome_to_machine_learning\\level 3\\cnn dataset\\my_emotion_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfdee847-c379-4b97-b5f2-0471e657f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.25,\n",
    "    zoom_range=[0.7,1.3],\n",
    "    brightness_range=[0.7,1.3],\n",
    "    channel_shift_range=20,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7845381b-01b3-4143-ac61-d293f2f6a1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1120 images belonging to 7 classes.\n",
      "Found 280 images belonging to 7 classes.\n",
      "Detected Classes: {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n"
     ]
    }
   ],
   "source": [
    "train_gen = datagen.flow_from_directory(\n",
    "    base_path,\n",
    "    target_size=(224,224),\n",
    "    batch_size=16,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    base_path,\n",
    "    target_size=(224,224),\n",
    "    batch_size=16,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "num_classes = train_gen.num_classes\n",
    "print(\"Detected Classes:\", train_gen.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e29fff-3bd6-49ce-8361-d743f9d5cc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224,224,3))\n",
    "base_model.trainable = False  # freeze base layers\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "x = Dropout(0.4)(x)\n",
    "output = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c735d1e1-05d8-4748-a32c-779b711aa07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86b8a60b-db2f-4428-b69a-fab4e7893c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(factor=0.2, patience=3, min_lr=1e-6)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "513f8e9a-f7ce-4a35-80da-25732feb48fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shivam\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step - accuracy: 0.1553 - loss: 2.2039"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shivam\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 466ms/step - accuracy: 0.1555 - loss: 2.2024 - val_accuracy: 0.2500 - val_loss: 1.8370 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 392ms/step - accuracy: 0.2087 - loss: 1.9373 - val_accuracy: 0.4179 - val_loss: 1.6798 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 384ms/step - accuracy: 0.2825 - loss: 1.7763 - val_accuracy: 0.4214 - val_loss: 1.6189 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 382ms/step - accuracy: 0.3757 - loss: 1.6682 - val_accuracy: 0.4857 - val_loss: 1.5587 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 391ms/step - accuracy: 0.3751 - loss: 1.6577 - val_accuracy: 0.5357 - val_loss: 1.5003 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 301ms/step - accuracy: 0.3993 - loss: 1.5875 - val_accuracy: 0.5536 - val_loss: 1.4626 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 260ms/step - accuracy: 0.4640 - loss: 1.4948 - val_accuracy: 0.6179 - val_loss: 1.3048 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 258ms/step - accuracy: 0.4817 - loss: 1.4515 - val_accuracy: 0.5964 - val_loss: 1.2866 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 259ms/step - accuracy: 0.4974 - loss: 1.4050 - val_accuracy: 0.6286 - val_loss: 1.2679 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 257ms/step - accuracy: 0.4954 - loss: 1.3944 - val_accuracy: 0.5893 - val_loss: 1.2040 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 257ms/step - accuracy: 0.5121 - loss: 1.3327 - val_accuracy: 0.6214 - val_loss: 1.1714 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 259ms/step - accuracy: 0.5698 - loss: 1.2716 - val_accuracy: 0.6357 - val_loss: 1.2013 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 257ms/step - accuracy: 0.5241 - loss: 1.3077 - val_accuracy: 0.6286 - val_loss: 1.1869 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 257ms/step - accuracy: 0.5666 - loss: 1.2506 - val_accuracy: 0.6893 - val_loss: 1.0504 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 258ms/step - accuracy: 0.5831 - loss: 1.1769 - val_accuracy: 0.6714 - val_loss: 1.0753 - learning_rate: 1.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 259ms/step - accuracy: 0.5523 - loss: 1.2484 - val_accuracy: 0.6571 - val_loss: 1.1174 - learning_rate: 1.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 257ms/step - accuracy: 0.5838 - loss: 1.1935 - val_accuracy: 0.7000 - val_loss: 1.0459 - learning_rate: 1.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 257ms/step - accuracy: 0.6157 - loss: 1.1223 - val_accuracy: 0.6786 - val_loss: 1.0129 - learning_rate: 1.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 258ms/step - accuracy: 0.6120 - loss: 1.1318 - val_accuracy: 0.6929 - val_loss: 1.0194 - learning_rate: 1.0000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 259ms/step - accuracy: 0.5816 - loss: 1.1310 - val_accuracy: 0.6821 - val_loss: 1.0093 - learning_rate: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "138f7166-3523-43f1-9055-62ac003a868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-20]: \n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df6f641c-a9a6-484a-be8a-a8b2878e62aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 295ms/step - accuracy: 0.3967 - loss: 1.6919 - val_accuracy: 0.7500 - val_loss: 0.9291 - learning_rate: 1.0000e-05\n",
      "Epoch 2/15\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 269ms/step - accuracy: 0.5098 - loss: 1.3266 - val_accuracy: 0.6250 - val_loss: 1.0592 - learning_rate: 1.0000e-05\n",
      "Epoch 3/15\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 271ms/step - accuracy: 0.5101 - loss: 1.2690 - val_accuracy: 0.7143 - val_loss: 0.9280 - learning_rate: 1.0000e-05\n",
      "Epoch 4/15\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 272ms/step - accuracy: 0.5516 - loss: 1.2044 - val_accuracy: 0.6821 - val_loss: 0.9813 - learning_rate: 1.0000e-05\n",
      "Epoch 5/15\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 268ms/step - accuracy: 0.5734 - loss: 1.1712 - val_accuracy: 0.7071 - val_loss: 0.9221 - learning_rate: 1.0000e-05\n",
      "Epoch 6/15\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 269ms/step - accuracy: 0.6046 - loss: 1.1414 - val_accuracy: 0.7107 - val_loss: 0.9069 - learning_rate: 1.0000e-05\n",
      "Epoch 7/15\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 270ms/step - accuracy: 0.6006 - loss: 1.0784 - val_accuracy: 0.7107 - val_loss: 0.8737 - learning_rate: 1.0000e-05\n",
      "Epoch 8/15\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 268ms/step - accuracy: 0.6093 - loss: 1.0571 - val_accuracy: 0.7107 - val_loss: 0.8915 - learning_rate: 1.0000e-05\n",
      "Epoch 9/15\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 270ms/step - accuracy: 0.6320 - loss: 1.0003 - val_accuracy: 0.7071 - val_loss: 0.9001 - learning_rate: 1.0000e-05\n",
      "Epoch 10/15\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 268ms/step - accuracy: 0.6800 - loss: 0.9756 - val_accuracy: 0.7286 - val_loss: 0.8571 - learning_rate: 1.0000e-05\n",
      "Epoch 11/15\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 268ms/step - accuracy: 0.6501 - loss: 0.9368 - val_accuracy: 0.6929 - val_loss: 0.8730 - learning_rate: 1.0000e-05\n",
      "Epoch 12/15\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 271ms/step - accuracy: 0.6928 - loss: 0.8898 - val_accuracy: 0.7393 - val_loss: 0.7826 - learning_rate: 1.0000e-05\n",
      "Epoch 13/15\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 277ms/step - accuracy: 0.6804 - loss: 0.9187 - val_accuracy: 0.7250 - val_loss: 0.7434 - learning_rate: 1.0000e-05\n",
      "Epoch 14/15\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 269ms/step - accuracy: 0.7020 - loss: 0.8742 - val_accuracy: 0.7571 - val_loss: 0.7703 - learning_rate: 1.0000e-05\n",
      "Epoch 15/15\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 283ms/step - accuracy: 0.6870 - loss: 0.9172 - val_accuracy: 0.7214 - val_loss: 0.8046 - learning_rate: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "history_finetune = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=15,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc57161b-df0e-4c14-bdd3-dd652e5e2f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved ✅\n"
     ]
    }
   ],
   "source": [
    "model.save(\"emotion_transfer.keras\")\n",
    "print(\"Model Saved ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b039271-dd00-46c6-aef2-faf39b888189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
